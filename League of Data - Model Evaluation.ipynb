{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# League of Data - Model Evaluation\n",
    "\n",
    "En el Notebook anterior, **League of Data - Data Prepropocessing** se realizó todo el proceso de limipieza de la información y se generó un conjunto de datos basado en cocientes para poder comparar dos equipos y así caracterizar una partida.\n",
    "\n",
    "En este Notebook nos enfocaremos en analizar la estructura del conjunto de datos y encontrar por medio de este, el modelo que mejor prediga cual es el equipo que resultará victorioso en una partida.\n",
    "\n",
    "Esta meta se alcanzó cumpliendo los siguientes objetivos.\n",
    "\n",
    "* Visualización del estado de los datos.\n",
    "* Métodología de validación y Medidas de error.\n",
    "* Entrenamiento de modelos.\n",
    "* Analísis de características.\n",
    "* Selección y extraccion de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZACIÓN DEL ESTADO DE LOS DATOS\n",
    "\n",
    "Ahora que se tiene el conjunto de datos a entrenar, siempre es una buena ayuda conocer como estan distribuidos los datos, como estos se ven en el espacio o en un plano. El ejercicio que se desarrolló tiene la ventaja de que es un problema de clasificación de tan solo 2 clases, sin embargo, como hemos dicho anteriormente, tiene 33 variables lo cual hace imposible visualizar toda la información sin recurrir a alguna técnica de reducción de dimensiones.\n",
    "\n",
    "El proceso de reducción de dimensiones previo a la visualización de los datos se realizó con dos técnicas **_Principal component analysis (PCA)_** y **_t-Distributed Stochastic Neighbor Embedding (t-SNE)_**. Como la intención es poder identificar como se distribuyen los datos en el plano y en el espacio, entonces, para cada una de las técnicas se probó reduciendo las características a dos y tres.\n",
    "\n",
    "**PCA:** Es una técnica no supervisada de extracción de características la cual tiene por objetivo conservar la mayor cantidad de información que sea posible. Lo que hace PCA es realizar una transformación de los datos originales a un nuevo conjunto de varibles, estos conjuntos se ordenan de modo que con unas pocas combinaciones se retengan la mayor cantidad de _variación_ presente en el conjunto **original** de variables.\n",
    "\n",
    "![PCA 2D](./Images/quotients_pca-2D.png)\n",
    "Como se puede observar en la gráfica, el conjunto de datos esta muy concentrado en un solo punto, tanto que pareciera que solo existe una clase, lo que nos atreveriamos a decir es que la clasificación va a ser un trabajo bastante complicado.\n",
    "\n",
    "![PCA 3D](./Images/quotients_pca-3D.png)\n",
    "Esta gráfica nos confirma aún más el estado en el que se encuentran los datos, la clase 0, correspondiente a las derrotas casi no se logra identificar.\n",
    "\n",
    "**t-SNE:** Es una técnica no lineal de reducción de dimensiones, en términos muy vagos, **t-sne** lo que hace es formar una distribución de probabilidad sobre los datos, de tal manera que aquellos que son similares tienen una alta probabilidad de pertenecer a un mismo conjunto, mientras que los datos diferentes tienen una probabilidad extremadamente pequeña.\n",
    "\n",
    "![t-SNE 2D](./Images/quotients_tsne-2D.png)\n",
    "Con esta gráfica notamos nuevamente que ambas clases presentan un comportamiento similar, todas tienden a juntarse en el mismo lugar y la clase de las victorias intenta sobreponerse a la clase que representa las derrotas, realizar la discriminación va a ser un poco complicado.\n",
    "\n",
    "![t-SNE 3D](./Images/quotients_tsne-3D.png)\n",
    "Esta gráfica nos aporta nueva información en comparación a las 3 anteriores. La clase que representa la derrota tiene una mayor presencia, opacando casi por completo a la clase que representa las victorias, sin embargo, podemos notar que la separación de las clases sigue siendo un reto.\n",
    "\n",
    "De las anteriores gráficas podemos concluir que es muy poco probable obtener resultados de clasificación por encima del 90%, ya que como se nota, la clases estan muy interrelacionadas, hasta el punto de no tener un solo sector en el cual se identifique a solo una de ellas.\n",
    "\n",
    "Sin embargo, este paso se realiza para que podamos tener una idea de en que estado se encuentran los datos, los resultados definitivos se obtienen a partir del siguiente paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METODOLOGÍA DE VALIDACIÓN Y MEDIDAS DE ERROR\n",
    "\n",
    "Las métodologías de validación son aquellas estrategías que nos permiten tener una estimación sobre el comportamiento del sistema ante nuevas muestras y con base en esto determinar cual configuración de parámetros es la que mejores resultados nos entrega. En otras palabras, el mejor parámetro no es aquel que mejor se comporta con las muestras del conjunto de datos, sino aquel que mejor se comporta con las muestras desconocidas. \n",
    "\n",
    "Dado que cada problema de aprendizaje de máquina cuenta con un conjunto de muestras, las metodologías de validación nos permite separar las datos, de tal manera que se usa una parte para entrenar el modelo y otra para validarlo. Este proceso nos ayuda a obtener buenos resultados estadísticos sobre el desempeño del modelo.\n",
    "\n",
    "Para efectos del problema que aquí se trata se decidió usar la validación Boostrap, con una distribución de 80% de los datos para entrenar y 20% para validar, siendo un poco más exactos, de las 5426 muestras de cocientes disponibles, 4341 se usarón para entrenar el modelo y 1085 para validarlo.\n",
    "\n",
    "Dados estos porcentajes, el procedimiento que usa esta metodología es, seleccionar de manera aleatoria las muestras a cada uno de los conjuntos, luego se entrena y valida el modelo para después volver a seleccionar aleatoriamente conjuntos diferentes. Este procedimiento se repetirá 100 veces.\n",
    "\n",
    "Teniendo en cuenta todo esto, para poder determinar cual modelo es el que mejor se ajusta a nuestro problema, necesitamos contar con una medida que nos indique que tan bueno es, para esto existen las medidas de error.\n",
    "\n",
    "En un problema de clasificación como el que estamos tratando aquí, el error se da si predecimos que una muestra pertenece a una clase cuando realmente pertenece a otra, esto nos muestra que el error no tiene un grado, sino que el error existe o no existe.\n",
    "\n",
    "Normalmente, en cualquier problema de aprendizaje de máquina se usa la eficiencia como medida de error, pero en problemas de clasificación es un arma de doble filo porque dada la definición que esta tiene, se asume que las clases poseen igual número de muestras, cuando sabemos que dicha situación no se cumple. Esto permite que la eficiencia quede sesgada a la clase con mayor número de muestras. Por esta razón, es importante tener en cuenta otro tipo de medidas de error, para este problema utilizaremos 4 medidas _sensiblidad, especificidad, eficiencia y presición_.\n",
    "\n",
    "Para obtener los resultados de estas medidas, recurriremos a una herramienta llamada matriz de confusión, la cual esta definida de la siguiente manera.\n",
    "\n",
    "![Matriz de confusión](./Images/cm.png)\n",
    "\n",
    "Por medio de esta herramienta podemos definir las medidas del error. \n",
    "\n",
    "**Sensibilidad:** Porcentaje de positivos bien clasificados $\\frac{TP}{TP+FN}$.\n",
    "\n",
    "**Especificidad:** Porcentaje de negativos bien clasificados $\\frac{TN}{TN+FP}$.\n",
    "\n",
    "**Eficiencia:** Porcentaje de aciertos del modelo $\\frac{TP+TN}{TP+TN+FP+FN}$.\n",
    "\n",
    "**Precisión:** Porcentaje de positivos bien predichos $\\frac{TP}{TP+FP}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO DE LOS MODELOS\n",
    "\n",
    "Después de haber analizado un poco la distribución de los datos, nos dimos cuenta que no se logra percibir un patrón de comportamiento en ellos, lo cual, además de complicarnos la discriminación de las clases, también nos pone difícil la tarea de seleccionar el modelo más pertinente. \n",
    "\n",
    "Dado lo anterior se decidió analizar el problema con 8 modelos distintos (Regresión Lineal, Ventana de Parzen, KNN, Random Forest, XGBoost, Redes Neuronales, Redes Neuronales Profundas, Máquinas de Soporte Vectorial), cada uno de ellos tiene una serie de hiperparámetros que dependiendo de su configuración presentan resultados diferentes. Por lo tanto, se ha establecido una malla de valores en cada modelo. \n",
    "\n",
    "Para cada configuración de hiperparámetros en su respectivo modelo, se realizó la misma metodología de validación con iguales condiciones, esto con el fin tener un punto en común de evaluación.\n",
    "\n",
    "Dado que algunos modelos cuentan con una cantidad considerable de configuraciones para sus hiperparámetros, los resultados que se expondrán en este Notebook serán aquellos que presenten el mejor resultado. Estos resultados se mostrarán por medio de dos gráficas,  La Curva ROC y el consolidado de las medidas de error.\n",
    "\n",
    "* **Regresión Linear**\n",
    "\n",
    "Es considerado el modelo más elemental dentro del aprendizaje de máquina, sin embargo, se ha demostrado que bajo ciertos escenarios presenta unos grandes resultados. \n",
    "\n",
    "El hiperparámetro que se tuvo en cuenta para este modelo es el grado del polinomio y los valores con que se probó fueron: _polinomio grado 1 y 2_, siendo el polinomio de grado 1 la mejor configuración para este modelo.\n",
    "\n",
    "| Curva ROC                                  | Medidas de Error                                     |\n",
    "| :----------------------------------------: | :--------------------------------------------------: |\n",
    "| ![logistic_ROC](./Images/logistic_ROC.png) | ![logistic_Measures](./Images/logistic_Measures.png) |\n",
    "\n",
    "\n",
    "* **k-Nearest Neighbors (KNN)**\n",
    "\n",
    "Este es un modelo que evalua todas las muestras que contiene el conjunto de datos y con base en alguna medida de similaridad predice para un valor para una nueva muestra. Para los problemas de clasificación como este, se evalua cuantas muestras son las más cercanas a la nueva y por medio de la _moda_ se identifica a cual clase pertenece.\n",
    "\n",
    "Los hiperparámetros con los que cuenta este modelo son el número de vecinos cercanos y la medida de distancia. Para el primero se tomó en cuenta los valores 1,2,3,4,5,10 y 100; en cuanto al segundo, solo se tomó en cuenta una configuración y fue la distancia euclidiana.\n",
    "\n",
    "El mejor resultado se obtuvo con la configuración de 100 vecinos.\n",
    "\n",
    "| Curva ROC                                  | Medidas de Error                                     |\n",
    "| :----------------------------------------: | :--------------------------------------------------: |\n",
    "| ![knn_ROC](./Images/knn_ROC.png) | ![knn_Measures](./Images/knn_Measures.png) |\n",
    "\n",
    "\n",
    "* **Ventana de Parzen**\n",
    "\n",
    "Este modelo es muy similar al anteriormente mencionado (KNN), la diferencia es que en este modelo no se da la definición de \"_vecindad_\" (Área variable definida por los puntos más cercanos), sino que por medio de una función Kernel genera una función de densidad de probabilidad. \n",
    "\n",
    "El hiperparámetro que se configura en este modelo es el ancho de la ventana, en otras palabras se modifica el peso que se le asigna a las muestras, entre más cerca, más peso asigna y mientras más lejos menos peso. La malla de valores asignada fue 0.05, 0.1, 0.5, 1, 10.\n",
    "\n",
    "El mejor resultado se dío con la ventana de ancho 0.5.\n",
    "\n",
    "![parzenwindow_Measures](./Images/parzenwindow_Measures.png)\n",
    "\n",
    "\n",
    "##### Comité de máquinas\n",
    "\n",
    "Una práctica muy usada en el aprendizaje de máquina son los _comités de máquinas_, por medio de este concepto se busca que la responsabilidad de decición de un modelo no se limite unicamente a un estimador, sino que se combine la decisión de varios estimadores para obtener de allí la solución definitiva. Dos de las técnicas más conocidas son el **Bagging** y el **Boosting**. \n",
    "\n",
    "La primer técnica lo que hace es, tomar el conjunto de entrenamiento  y realizar una serie de muestreos sobre el, luego, entrena un clasificador diferente para cada uno de los conjuntos anteriormente seleccionados. La decisión final se toma dependiendo de si el problema es de clasificación o de regresión, para un problema de clasificación se utiliza la moda, mientras que para un problema de regresión se usa el promedio.\n",
    " \n",
    "La segunda técnica consiste en tomar todo el conjunto de entrenamiento y con él entrenar un modelo, después de esto se seleccionan aquellas muestras que quedaron mal interpretadas (estas muestras se les conoce como muestras pesadas) mas algunas otras muestras (tomadas aleatoriamente del conjunto de entrenamiento) y con ellas se vuelve a entrena el modelo.\n",
    "\n",
    "Para el problema que estamos desarrollando se tuvo en cuenta un modelo que implementa la técnica de Bagging y otro que implementa la técnica de Boosting.\n",
    "\n",
    "* **Random Forest**\n",
    "\n",
    "Este modelo es un comité de árboles de decisión el cual usa la técnica de Bagging con un característica adicional. La diferencia fundamental es que en la técnica normal de Bagging, se entrenan muestras aleatroias pero todas con las mismas variables, mientras que en el Random Forest las muestras son aleatorias para cada árbol y además las características que se evaluan en los respectivos árboles también se escogen al azar.\n",
    "\n",
    "El hiperparámetro que se evalua en este modelo es la cantidad de árboles de decisión en cada estimador, la malla de valores escogida fue 10, 20, 30, 40, 50 y 500 árboles.\n",
    "\n",
    "El mejor resultado para este modelo se consiguió con la configuración de 500 árboles.\n",
    "\n",
    "| Curva ROC                                  | Medidas de Error                                     |\n",
    "| :----------------------------------------: | :--------------------------------------------------: |\n",
    "| ![forest_ROC](./Images/forest_ROC.png) | ![forest_Measures](./Images/forest_Measures.png) |\n",
    "\n",
    "\n",
    "* **XGBoost**\n",
    "\n",
    "Como el modelo anterior, este modelo también es un comité de árboles de decisión pero en este caso usa la técnica de Boosting. \n",
    "\n",
    "Para este modelo, el hiperparámetro a configurar y la malla de valores son iguales, el hiperparámetro es la cantidad de árboles y la malla de valores fue 10, 20, 30, 40, 50 y 500 árboles.\n",
    "\n",
    "El mejor resultado se encontró con la configuración de 40 árboles\n",
    "\n",
    "| Curva ROC                                  | Medidas de Error                                     |\n",
    "| :----------------------------------------: | :--------------------------------------------------: |\n",
    "| ![xgboost_ROC](./Images/xgboost_ROC.png) | ![xgboost_Measures](./Images/xgboost_Measures.png) |\n",
    "\n",
    "\n",
    "##### Redes Neuronales\n",
    "\n",
    "* **MLP**\n",
    "\n",
    "El perceptrón multicapa (MLP) es un tipo de red neuronal artificial que utiliza como unidad básica y principal al perceptrón. Este elemento no es más que un clasificador lineal que toma valores reales como entrada, calcula una combinación lineal de estas entradas y produce una salida, 1 si el resultado es mayor que algún umbral y -1 en otro caso.\n",
    "\n",
    "En general, las redes neuronales tienen algo que se conoce como capas, estas capas se distribuyen en:\n",
    "* Entrada: Recibe las características o variables a ser evaluadas.\n",
    "* Ocultas: Capas de perceptrones.\n",
    "* Salida: Donde se entregan los resultados.\n",
    "\n",
    "Una de las principales características de la red MLP es que siempre los perceptrones de una capa estan conectados completamente a todos los perceptrones de la siguiente capa. \n",
    "\n",
    "Otra característica muy importante, es que la red se entrena hacia adelante (FeedForward) pero se propaga hacía atras (BackPropagation) y siempre lo hacen pasando por cada una de las capas anteriormente mencionadas.\n",
    "\n",
    "Para este modelo de perceptrón multicapa se configuraron tres hiperparámtros. El primero es el número de iteraciones al cual se le dió un valor constante de 500. El segundo es el número de capas ocultas el cual esta en 1, 2 y 3 capas. El tercer hiperparámtro es el número de neuronas por capa oculta, la malla de valores se explica en la siguiente tabla.\n",
    "\n",
    "| Neuronas en la capa 1 | Neuronas en la capa 2 | Neuronas en la capa 3 |\n",
    "| :-------------------: | :-------------------: | :-------------------: | \n",
    "| 16 | - | - |\n",
    "| 32 | - | - |\n",
    "| 48 | - | - |\n",
    "| 64 | - | - |\n",
    "| 16 | [4,8,12] | - |\n",
    "| 32 | [10,20,30] | - |\n",
    "| 48 | [15,30,45] | - |\n",
    "| 64 | [20,40,60] | - |\n",
    "| 16 | [4,8,12] | [2,3],[3,6],[5,10] |\n",
    "| 32 | [10,20,30] | [4,8],[9,18],[14,28] |\n",
    "| 48 | [15,30,45] | [6,12],[14,28],[21,42] |\n",
    "| 64 | [20,40,60] | [9,18],[16,32],[29,58] |\n",
    "\n",
    "El mejor resultado, se obtuvo con la configuración de 3 capas ocultas y 32 neuronas en la primer capa, 20 neuronas en la segunda y 9 neuronas en la tercera.\n",
    "\n",
    "| Curva ROC                                  | Medidas de Error                                     |\n",
    "| :----------------------------------------: | :--------------------------------------------------: |\n",
    "| ![mlp_ROC](./Images/mlp_ROC.png) | ![mlp_Measures](./Images/mlp_Measures.png) |\n",
    "\n",
    "\n",
    "* **Deep NN**\n",
    "\n",
    "Este modelo es muy usado en la actualidad debido a que los costos de ejecución se han acortado, entre otras cosas, gracias al creciente uso de las GPU. La diferencia principal con una red neuronal tradicional tipo MLP (como el modelo anteriormente descrito) es que las redes neuronales profundas usan una enorme cantidad de capas ocultas, además dichas capas pueden ser de diferentes tipos como _Densas, combolucionales, activación, entre otras_.\n",
    "\n",
    "Para este modelo no se configuró una malla de valores, los hiperparámetros que se tuvieron en cuenta solo tomaron un valor. El número de capas ocultas fue 9, el tipo de capa oculta fue _Densa_ (completamente conectada), la función de activación en cada capa fue la función _relu_, el número de neuronas por capas oculta fue 32, 28, 24, 20, 16, 12, 8, 4 y 1 respectivamente y finalmente el número de épocas fue 500.\n",
    "\n",
    "Los resultados obtenidos para las medidas de error fueron los siguientes.\n",
    "![deepNN_Measures](./Images/deepNN_Measures.png) |\n",
    "\n",
    "\n",
    "##### Máquinas de Soporte Vectorial\n",
    "\n",
    "Un concepto que es muy importante definir es la diferencia entre el riesgo estructural y el riesgo empírico. El primer tipo de riesgo consiste en elegir la mejor frontera de decisión para la cual la probabilidad de error a futuro sea menor, osea, es encontrar la mejor frontera de entre todas las posibles en la cual el error con las muestras de validación sea mínimo. Por otra parte el riesgo empírico se trata sobre medir el error con base en el conjunto de entrenamiento y esperar que haya un comportamiento similar o igua en el conjunto de validación.\n",
    "\n",
    "Este modelo se basa en el riesgo estructural porque nos permite encontrar la mejor solución a un problema utilizando como criterio de ajuste la maximización del márgen. El márgen es la distancia mas corta entre la frontera de decisión y cualquier muestra de nuestro conjunto de datos. En conclusión una máquina de soporte vectorial siempre buscará la frontera que mejor separe las muestras.\n",
    "\n",
    "Pero entonces **¿Qué es un vector de soporte?** Los vectores de soporte son todas aquellas muestras que tocan el límite del márgen de decisión, aquellas muestras con un grado de incertidumbre muy alto, las difíciles de clasificar. Con estas muestras complicadas es que se crea la frontera de decisión.\n",
    "\n",
    "Para este problema se usarón máquinas de soporte vectorial con dos configuraciones diferentes, una con kernel lineal y la otra con kernel radial. La primer implementación es similar al modelo de regresión logistica, mientras que la segunda implementación es similar al modelo de ventana de parzen (por el tipo kernel que se usa).\n",
    "\n",
    "* **Kernel Lineal**\n",
    "\n",
    "El hiperparámtro que se configuró para este modelo fue el Box Constraint y la malla de valores que se le asignó esta basado en potencias de 10, empezando desde 0.01 y terminando en 100.\n",
    "\n",
    "El mejor resultado que se obtuvo fue con un valor de 0.01 para el Box Constraint.\n",
    "\n",
    "| Curva ROC                                  | Medidas de Error                                     |\n",
    "| :----------------------------------------: | :--------------------------------------------------: |\n",
    "| ![svmLinear_ROC](./Images/svmLinear_ROC.png) | ![svmLinear_Measures](./Images/svmLinear_Measures.png) |\n",
    "\n",
    "* **Kernel RBF**\n",
    "\n",
    "Para esta configuración de kernel radial se configuraron dos hiperparámetros, el primero fue el Box Constraint y el segundo el valor gamma, este cumple un papel similar al hiperparámetro configurado en el modelo de ventana de parzen. Nuevamente la malla de valores esta basada en potencias de 10 empenzando desde 0.01 y terminando en 100, tanto para el Box Constraint como para el gamma. Es importante aclarar que para cada valor del Box Constraint el hiperparámetro gamma toma los 5 valores anteriormente mencionados.\n",
    "\n",
    "| Box Constraint | Gamma | \n",
    "| :------------: | :---: |\n",
    "| 0.01 | 0.01|\n",
    "| 0.01 | 0.1 |\n",
    "| 0.01 | 1   |\n",
    "| 0.01 | 10  |\n",
    "| 0.01 | 100 |\n",
    "| ---- | --- |\n",
    "| 100 | 0.01|\n",
    "| 100 | 0.1 |\n",
    "| 100 | 1   |\n",
    "| 100 | 10  |\n",
    "| 100 | 100 |\n",
    "\n",
    "El mejor resultado que se obtuvo fue con un valor de 1 para el Box Constraint y 10 para Gamma.\n",
    "\n",
    "| Curva ROC                                  | Medidas de Error                                     |\n",
    "| :----------------------------------------: | :--------------------------------------------------: |\n",
    "| ![svmGaussian_ROC](./Images/svmGaussian_ROC.png) | ![svmGaussian_Measures](./Images/svmGaussian_Measures.png) |\n",
    "\n",
    "\n",
    "\n",
    "Luego de haber usado cada uno de los modelos, podemos confirmar la hipotesis que nos planteamos en la etapa de analisis de distribución de los datos, no fue posible en ninguna de las medidas de error alcanzar el 90% de aciertos, de hecho, tampoco se alcanzó el 80% de aciertos.\n",
    "\n",
    "Otros factores que también se pueden interpretar son.\n",
    "\n",
    "* El porcentaje de positivos bien clasificados (Sensibilidad) es la medida que más fluctuación tiene, el modelo que peor predijó esta medida entregó un 51% de aciertos, mientras que el modelo con mejor resultado,  entregó un 78% de aciertos.\n",
    "* La especificidad fue la medida con peores resultados, la mayor parte de los modelos entregó valores entre el 50% y el 55%, solo para un modelo se obtuvo un resultado superior al promedio (76%). Esto resultados nos quieren decir que las muestras de la clase que representa las derrotas en una partida son muy dificiles de clasificar, en otras palabras, al sistema le da más dificultad determinar cuando un equipo será derrotado.\n",
    "* La exactitud es la medida de error más constante en todos los modelos, la mayor parte de las ejecuciones se obtuvieron resultados entre 65% y 68%.\n",
    "* La presición es la medida de error con mejores resultados, siempre por encima del 66% con un tope máximo del 77%. Lo que podemos concluir de esto es que el sistema tiene una gran capacidad para predecir la clase que representa las victorias, porque muy pocas veces da como ganador a un equipo que perdió.\n",
    "* Se puede concluir que los modelos que mejores resultados entregan son el XGBoost y la red neuronal profunda. Cada uno tiene falencias en una medida de error, el XGBoost tiene problemas con la especificidad, mientras que la red neuronal tiene problemas con la sensibilidad, sin embargo en el resto de medidas entregan muy buenos resultados. \n",
    "    * Si se escogiera un solo modelo como ganador, este, sería el XGBoost, porque apesar de tener resultados similares a la red neuronal, este tiene mejores tiempos de ejecución.\n",
    "    * No es de extrañar que estos dos modelos entreguen los mejores resultados, si se hace una revisión del estado actual de las competencias de Machine Learning, encontramos que en la mayorias de los casos estos modelos son los más utilizados y también los que mejores resultados entregan.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de Características\n",
    "\n",
    "Como se ha podido notar en la ejecución de los modelos, los resultados obtenidos no son tan buenos como quisieramos que fueran. En el análisis de distribución de los datos expusimos una razón por la cual esto podría pasar, sin embargo, pueden existir otra razón relacionada con las características, y es que quizás estas no tengan buena capacidad predictiva.\n",
    "\n",
    "A pesar de que el problema no sufre de lo que conocemos como la maldición de la dimensionalidad (más variables que muestras), es una buena practica aplicar técnicas de reducción de dimensión para determinar si existen características con mala capacidad predictiva, de tal modo que si esto se presenta, se puedan eliminar las características con problemas y asi mejorar los resultados.\n",
    "\n",
    "En esta sección se realizará un análisis de caracterı́sticas por medio del **Coeficiente de correlación de Pearson** y **Cociente discriminante de fisher** para hallar las variables candidatas a ser eliminadas.\n",
    "\n",
    "* **Coeficiente de correlación de Pearson**\n",
    "\n",
    "El coeficiente de correlación de Pearson es un ı́ndice que mide la relación lineal entre dos variables aleatorias. Este\n",
    "ı́ndice es independiente de la unidad de medida y toma valores entre -1 y 1, donde:\n",
    "* $\\rho = 1$ Existe correlación positiva perfecta.\n",
    "* $0 < \\rho < 1$ Existe correlación positiva.\n",
    "* $\\rho = 0$ No existe correlación.\n",
    "* $-1 < \\rho < $ Existe correlación negativa.\n",
    "* $\\rho = -1$ Existe correlación negativa perfecta.\n",
    "\n",
    "Es importante aclarar que este coeficiente entrega muy buenos resultados cuando las características muestran un comportamienta lineal, cuando esto no ocurre, los resultados no son tan buenos. \n",
    "\n",
    "![Pearson_Analysis](./Images/Pearson_Analysis.png)\n",
    "\n",
    "Lo que se busca con este coeficiente es que la correlación entre las características sea menor puesto que así habrá más independencia, mientras que la correlación entre las características y la variable de salida sea mucho mayor porque esto se traduce en alta capacidad predictiva.\n",
    "\n",
    "Algunas observaciones que podemos hacer de la gráfica anterior son:\n",
    "* En general las características de la 2 a la 9 muestran diferentes grados de correlación, algunos con alto grado positivo y otras con alto grado negativo, sin embargo, de este conjunto de características, aquellas con mayor capacidad predictiva son la 4,5,6 y 8. Estas características corresponden al promedio de tiempo que tardan en ganar estando tanto en el lado azul como en el lado rojo, el promedio de tiempo que tardan en ganar una partida y el promedio de veces que mueren sus campeones. Si observamos la gráfica, nos damos cuenta que dichas variables presentan una correlación negativa, lo cual tiene mucho sentido puesto que entre menos tiempo un equipo acumule en estas características, mejores serán sus características.\n",
    "* Las características 7 y 9, que corresponden al promedio de asesinatos (mean_kills) y promedio de asistencias (mean_assis) tienen una alta correlación positiva, luego al compararlas con la variable de salida notamos que tienen un grado similar de correlación. Por tanto, podemos concluir que alguna de estas dos características es candidata a ser eliminada. Esta relación tiene mucho sentido, porque cada vez que un equipo asesina un enemigo, pueden haber de 0 a 5 asistencias.\n",
    "* Las características 30, 31 y 32 presentan altos indices de correlación positiva, y al observar su relación con la variable de salida nos damos cuenta que su capacidad predictiva no es muy alta, por tanto, podemos intuir que por lo menos una de estas características son candidatas a ser eliminadas. Estas variables, corresponden a promedio de centinelas puestos y promedio de subditos y centinelas asesinados.\n",
    "* La característica 8 (Promedio de muertes) presenta altos indices de correlación negativa con las características 30, 31 y 32 (explicadas en la observación anterior). Esta relación tiene mucho sentido dentro de la estrategia del juego, ya que a un equipo que constantemente lo esten asesinando, el número de centinelas y subditos asesinados va a ser muy bajo. Por el contrario, mientras un equipo menos lo asesinen, más subditos y centinalas va a asesinar y poner.\n",
    "* Una observación muy relevante que se debe hacer de la gráfica, es que son muy pocas las características que tienen alta correlación con la variable de salida, lo cual nos augura una alta cantidad de variables a ser eliminadas.\n",
    "\n",
    "\n",
    "* **Cociente Discriminante de Fisher**\n",
    "\n",
    "Este es un indice que permite medir la capacidad discriminante de una o varias variables bajo la suposición de que una buena característica debe proporcionar baja dispersión intra-clase y alta dispersión entre-clases. A comparación del coeficiente anterior este no tiene un rango de valores limitado, por lo tanto, para poder hacer una evaluación más objetiva se puede calcular el cociente de Fisher para cada característica y normalizar el valor con respecto al máximo. Este resultado nos permite afirmar que, las características con mayor capacidad discriminante, serán aquellas que tengan índices por encima del 0.5.\n",
    "\n",
    "_Para efectos del problema que se esta tratando, se tendran en cuenta aquellas características con indice discriminante mayor a 0.4_.\n",
    "\n",
    "![fisher-discriminant_Analysis](./Images/fisher-discriminant_Analysis.png)\n",
    "\n",
    "De la gráfica anterior, podemos notar que tan solo 7 de las 32 características superan el márgen de 0.4 de capacidad discriminante y además tan solo 4 de las 7 superan el 0.5.\n",
    "\n",
    "De las características que superan el márgen podemos decir que dentro del juego definitivamente son variables que determinan la victoria o derrota de un equipo, por ejemplo, un equipo que tenga buen promedio de de torres destruidas (*mean_total_towers*) tiene altas posibilidades de llegar al nexo y ganar, además, si este mismo equipo destruye de  primero tres torres (*percentage_fthreetowers*) sus posibilidades de victoria son más altas.\n",
    "\n",
    "Por otro lado tenemos las caracteriscas que representan el KDA (**Kills, Deaths and Assits** ), en cierta medida, estas variables también son un factor importante a la hora de determinar una partida, los equipos con un bajo KDA tienen más posiblidades de perder que aquellos equipos con altos indices de KDA.\n",
    "\n",
    "Luego de haber analizado el índice de Pearson y el coeficiente de Fisher podemos afirmar que hay un gran número de variables candidatas a ser eliminadas, se espera un 68.7% de reducción. Con esto lo que si podremos asegurar es una reducción considerable en los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección y Extracción de características\n",
    "\n",
    "Después de haber analizado cuales características son candidatas a ser eliminadas, el paso siguiente es aplicar métodos de selección y extracción de características que nos permitan evaluar más detalladamente cada variable y los posibles efectos que tienen los subconjuntos de ellas en la predicción del resultado.\n",
    "\n",
    "La selección y extracción de características son procesos muy diferentes que atacan el problema desde varios puntos de vista.\n",
    "\n",
    "| Selección | Extracción |\n",
    "| --------- | --------- |\n",
    "| Toma un subconjunto de variables del conjunto completo | Toma todo el conjunto de muestras y los combina para producir un nuevo conjunto |\n",
    "| Las variables originales conservan su sentido | Su objetivo es proveer un método que permita condensar la información de todas las variables |\n",
    "| Las variables removidas eliminan costo de computación | Tienen menor perdida de información |\n",
    "| La carga computacional de los algoritmos suele ser elevada  | Normalmente son computacionalmente eficientes |\n",
    "| Se puede descartar información importante | Una vez transformados los datos, estos carecen de sentido |\n",
    "| - | Las variables originales son proyetadas en un nuevo espacio |\n",
    "\n",
    "\n",
    "* Selección de características\n",
    "\n",
    "Este proceso se efectuó por medio de **Selección Secuencial de Características (SFS)** el cual es un conjunto de métodos que tienen en cuenta todas las variables al mismo tiempo, mide su desempeño y evalua diferentes combinaciones de tal manera que al final quede un subconjunto de características con alta capacidad predictoria. \n",
    "\n",
    "SFS tiene en cuenta dos elementos muy importantes, la estrategía de búsqueda y la función objetivo. La estrategía de búsqueda permite explorar el espacio de características para encontrar el mejor subconjunto de una manera más eficiente, la función objetivo es la que valida la estrategía de búsqueda y la que nos permite decidir que tan bueno fue el conjunto elegido.\n",
    "\n",
    "La implementación del proceso SFS que se realizó para resolver este problema fue con base en la biblioteca [MLXtend](https://rasbt.github.io/mlxtend/), se usó la estrategía _búsqueda secuencial ascendente_, la función objetivo fue de tipo _wrapper_ y para esto se tuvo en cuenta 6 modelos diferentes (RandomForest, KNN, Regresión, MLP, máquinas de soporte vectorial y XGBoost), aquellos que presentan mayor compatibilidad con la biblioteca MXLtend. \n",
    "\n",
    "Normalmente cuando se usa una función objetivo de tipo _wrapper_ el subconjunto de datos carece de generalidad (capacidad de usar el subconjunto de características indistintas del modelo), es por esto que se decidió realizar el proceso usando estos 6 modelos. Además es muy importante aclarar que la configuración de los hiperparámetros de estos modelos esta definida por las configuraciones más exitosas.\n",
    "\n",
    "Los resultados obtenidos fueron los siguientes.\n",
    "\n",
    "* **Random Forest**: Con una configuración de 500 árboles se obtuvo un subconjunto de 15 características con un performance de 0.65. Las caraceterísticas seleccionadas fueron [0, 1, 2, 4, 5, 8, 10, 13, 15, 17, 19, 22, 24, 28, 29].\n",
    "\n",
    "![forest_fsf](./Images/FeatureSelection/Analysis/forest_fsf.png)\n",
    "\n",
    "* **KNN**: Con una configuración de 100 vecinos se obtuvo un subconjunto de 20 características con un performance de 0.62. Las caraceterísticas seleccionadas fueron [0, 2, 3, 5, 8, 10, 12, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30].\n",
    "\n",
    "![knn_fsf](./Images/FeatureSelection/Analysis/knn_fsf.png)\n",
    "\n",
    "* **KNN**: Con una configuración de 100 vecinos se obtuvo un subconjunto de 20 características con un performance de 0.62. Las caraceterísticas seleccionadas fueron [0, 2, 3, 5, 8, 10, 12, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30].\n",
    "\n",
    "![knn_fsf](./Images/FeatureSelection/Analysis/knn_fsf.png)\n",
    "\n",
    "* **Regresión**: Con una polinomio de grado 1 se obtuvo un subconjunto de 19 características con un performance de 0.66. Las caraceterísticas seleccionadas fueron [0, 1, 2, 4, 6, 10, 11, 12, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 29].\n",
    "\n",
    "![logistic_fsf](./Images/FeatureSelection/Analysis/logistic_fsf.png)\n",
    "\n",
    "* **MLP**: Con la configuración de 3 capas ocultas y 32 neuronas en la primer capa, 20 neuronas en la segunda y 9 neuronas se obtuvo un subconjunto de 8 características con un performance de 0.665. Las caraceterísticas seleccionadas fueron [2, 3, 11, 12, 14, 15, 18, 19].\n",
    "\n",
    "![mlp_fsf](./Images/FeatureSelection/Analysis/mlp_fsf.png)\n",
    "\n",
    "* **SVM Kernel Lineal**: Con un Box Constraint de 0.01 se obtuvo un subconjunto de 4 características con un performance de 0.658. Las caraceterísticas seleccionadas fueron [2, 16, 18, 25].\n",
    "\n",
    "![svmlinear_fsf](./Images/FeatureSelection/Analysis/svmlinear_fsf.png)\n",
    "\n",
    "* **SVM Kernel RBF**: Con un Box Constraint de 1 y un gamma de 10 se obtuvo un subconjunto de 21 características con un performance de 0.647. Las caraceterísticas seleccionadas fueron [1, 2, 3, 5, 6, 8, 9, 10, 12, 14, 16, 17, 18, 19, 20, 22, 24, 25, 26, 27, 29].\n",
    "\n",
    "![svmrbf_fsf](./Images/FeatureSelection/Analysis/svmrbf_fsf.png)\n",
    "\n",
    "* **XGBoost**: Con una configuración de 40 árboles se obtuvo un subconjunto de 27 características con un performance de 0.6675. Las caraceterísticas seleccionadas fueron [0, 1, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31].\n",
    "\n",
    "![xgboost_fsf](./Images/FeatureSelection/Analysis/xgboost_fsf.png)\n",
    "\n",
    "\n",
    "Luego de tener los subconjuntos arrojados por cada uno de los modelos, se contaron las ocurrencias de las características y se creó un subconjunto con todas aquellas características que aparecen 4 o más veces. En definitiva, el subconjunto a evaluar tiene 18 características que se mostrarán a continuación.\n",
    "\n",
    "![features-count](./Images/FeatureSelection/Analysis/features-count.png)\n",
    "\n",
    "\n",
    "Si comparamos el subconjunto definitivo con las variables candidatas a ser eliminadas del punto anterior, nos daremos cuenta que muchas de las características que inicialmente pensamos iban a ser eliminadas se mantuvieron en el subconjunto definitivo, por otro lado, aquellas que presentaban mayor grado de correlación como las variables 7, 31 y 32 no lograron pasar. Además, podemos notar como 5 de las 7 variables que el cociente de fisher nos arrojó con capacidad discriminante aún permanecen en el nuevo subconjunto.\n",
    "\n",
    "Ahora que se tiene el nuevo subconjunto de datos, el paso siguiente es evaluar nuevamente los modelos y verificar los resultados entregados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
